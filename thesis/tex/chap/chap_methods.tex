\chapter{Methods} \label{chap:methods}
The methods of this work are introduced here. Section \ref{sec:classification_method} describes the classification model and the learning method. In \cref{sec:network_pruning} the developed network pruning algorithm is introduced and also the dimensionality reduction in pruned networks is shown. Section \ref{sec:feature_selection} comes with the feature selection method using a procedure called \textit{pathing} and additionally, some ideas of how to use remaining weights in minimal structures are suggested. Finally, \cref{sec:speech_data_gathering} is devoted to the process of speech data gathering.

Section \ref{sec:classification_method} moreless specifies a generally known approach. Sections \ref{sec:network_pruning} - \ref{sec:feature_selection} are partially based on \citep{bulin_2016}, but the methods are further elaborated. Section \ref{sec:speech_data_gathering} is highly supported by \citep{smidl_pc}.

\section{Classification Method} \label{sec:classification_method}
The classification in this study is performed by dense feedforward neural networks. A general illustration of the network structure is in \cref{fig:methods:ff_neural_net}. Note that the structure is fully connected, meaning that each neuron is connected to all neurons in the next layer.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{ff_neural_net}
\caption{A general dense feedforward neural network.}
\label{fig:methods:ff_neural_net}
\end{figure}

The number of input units ($ n $) is determined by the problem dimension. The number of classes gives the number of output units ($ m $). In this study, we mostly use a simple hidden structure, usually with one hidden layer ($ q = 1 $).

\subsection*{Neuron Principle}
The behaviour of artificial neurons follows our understanding of how biological neurons work. One unit has multiple inputs and a single output. A model of neuron is shown in \cref{fig:methods:model_neuron}. The diagram complies with the following notation:

\begin{description}
\item[$ neuron_k^{(i)} $] : $ k^{th} $ neuron in $ i^{th} $ layer
\item[$ a_k^{(i)} $] : activity of $ k^{th} $ neuron in $ i^{th} $ layer
\item[$ w_{k,l}^{(i)} $] : weight of synapse connecting $ l^{th} $ neuron in $ (i-1)^{th} $ layer with $ k^{th} $ neuron in $ i^{th} $ layer
\item[$ b_k^{(i)} $] : bias connected to $ k^{th} $ neuron in $ i^{th} $ layer
\item[$ z_k^{(i)} $] : activation of $ k^{th} $ neuron in $ i^{th} $ layer
\item[$ f() $] : transfer function (\cref{eq:sigmoid}; \cref{fig:methods:transfer_functions})
\end{description}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{neuron_principle.png}
  \caption{A model neuron}
  \label{fig:methods:model_neuron}
\end{figure}

Assuming $ j $ being the number of neurons in $ (i-1)^{th} $ layer, the activation of $ neuron_k^{(i)} $ is computed as in \cref{eq:neuron_activation}

\begin{align} \label{eq:neuron_activation}
z_k^{(i)} = \displaystyle{\sum_{l=1}^{j} [a_l^{(i-1)} \cdot w_{k,l}^{(i)}]} + b_k^{(i)}
\end{align}

Then we get the neuron activity by mapping its activation into a finite interval using a transfer function $ f(\cdot) $ - see \cref{eq:neuron_activity}. 

\begin{align} \label{eq:neuron_activity}
a_k^{(i)} = f(z_k^{(i)})
\end{align}

The \textit{Sigmoid} function (\cref{eq:sigmoid}) keeps neuron activities in the $ [0, 1] $ interval and is used by default in this work. Alternatively, one could use the hyperbolic tangent ($ tanh(\cdot) $) function, which maps the input into the $ [-1, 1] $ interval.

\begin{align} \label{eq:sigmoid}
f(z) = \frac{1}{1 + e^{-z}}
\end{align}

The two basic transfer functions are shown in \cref{fig:methods:transfer_functions}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{transfer_functions.eps}
  \caption{Transfer functions: \textit{Sigmoid} and \textit{Tanh}}
  \label{fig:methods:transfer_functions}
\end{figure}

\subsection*{Notation}
The work of a neural network is all done by matrix calculations. Therefore we need to introduce a notation used in this study. Detailed examples of the itemized matrices can be found in \cref{app:implementation}.

\begin{description}[leftmargin=!,labelwidth=\widthof{\bfseries $ W^{(i)} $}]
\item[$ n $] : number of input neurons (problem dimension; size of one sample);
\item[$ m $] : number of output neurons (classes);
\item[$ p $] : number of samples;
\item[$ q $] : number of hidden layers;
\item[$ X $] : network input: $ n $-by-$ p $ matrix;
\item[$ W^{(i)} $] : $ r $-by-$ s $ matrix of weights for synapses connecting $ s $ neurons in $ (i-1)^{th} $ layer to $ r $ neurons in $ i^{th} $ layer;
\item[$ B^{(i)} $] : vector of $ r $ biases for $ r $ neurons in $ i^{th} $ layer;
\item[$ Z^{(i)} $] : $ r $-by-$ p $ matrix of activations for $ r $ neurons in $ i^{th} $ layer for all samples;
\item[$ A^{(i)} $] : $ r $-by-$ p $ matrix of activities of $ r $ neurons in $ i^{th} $ layer for all samples;
\item[$ \Delta^{(i)} $] : $ r $-by-$ p $ matrix of errors on neurons in $ i^{th} $ layer for all samples;
\item[$ Y $] : predicted network output for all samples: $ m $-by-$ p $ matrix ($ Y = A^{(q)} $)
\item[$ U $] : desired network output (targets) for all samples: $ m $-by-$ p $ matrix
\end{description}

\subsection*{Learning Algorithm}
The feedforward networks are trained by the common \textit{Backpropagation} method illustrated by the flowchart in \cref{fig:methods:backpropagation}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{backpropagation.png}
  \caption{Training process flowchart. $ T_1 $: Threshold for a terminating condition based on the prediction error. If the error is reduced to be lower, the learning process is stopped and the model is considered trained. $ T_2 $: Threshold for a terminating condition based on the number of epochs. The learning process is stopped after a specified number of epochs, no matter how successful the training has been.}
  \label{fig:methods:backpropagation}
\end{figure}

In case of feedforward neural networks, the goal is to find optimal values for two groups of parameters - \textit{weights} ($ W $) and \textit{biases} ($ B $). The key idea lies in the optimization method called \textit{Gradient Descent Algorithm (GDA)}.

At first, a batch of samples $ X $ is (forward) propagated through a network to get the network's prediciton $ Y $.

\begin{align}
Z^{(1)} &= W^{(1)} \cdot X + B^{(1)} \\
A^{(1)} &= f(Z^{(1)}) \\
A^{(i)} &= f(W^{(i)} \cdot A^{(i-1)} + B^{(i)}) \\
Y &= A^{(q)} = f(W^{(q)} \cdot A^{(q-1)} + B^{(q)})
\end{align}

Then, having the known targets (supervised learning), we compute a prediction error on every output neuron for every sample and store those errors in the $ m $-by-$ p $ matrix $ E $.

\begin{equation} \label{eq:prediction_error}
E = \frac{1}{2} ||Y - U||^2
\end{equation}

Now it is time to use \textit{GDA} to find such settings of $ W $ and $ B $ that makes $ E $ minimal. The details of the \textit{Backpropagation} procedure are well described in \citep{online:nnanddl}. The envelope of the algorithm includes these steps:

\begin{enumerate}
\item \textit{find the derivative of the transfer function (assuming Sigmoid)};
\begin{align} \label{eq:transfer_function_der}
f'(z) = f(z) \cdot (1-f(z))
\end{align}
\item \textit{backpropagate the prediction error throught the network;}
\begin{align} \label{eq:error_backprop}
\Delta^{(q+1)} &= (Y-U) \times f'[Z^{(q+1)}] \\
\Delta^{(i)} &= \left[\left[W^{(i+1)}\right]^T \cdot \Delta^{(i+1)}\right] \times f'[Z^{(i)}]
\end{align}
\item \textit{find the optimal parameter changes;}

Every sample $ \xi $ has a vote $ dW^{(i)}_{(\xi)} $ (resp. $ dB^{(i)}_{(\xi)} $) on how the parameters $ W^{(i)} $ (resp. $ B^{(i)} $) should change to get the minimal error and the result is then obtained as a compromise of those votes.

The index $ i $ indicates the layer. Consider $ \Delta^{(i)}_{(\xi)} $ be the $ \xi^{th} $ column of the $ \Delta^{(i)} $ matrix, which corresponds to the $ \xi^{th} $ sample. Analogically, $ A^{(i-1)}_{(\xi)} $ is the $ \xi^{th} $ column of the activation matrix $ A^{(i-1)} $ in the $ (i-1)^{th} $ layer. Then we get the votes as:
\begin{align} \label{eq:part_derivative}
dW^{(i)}_{(\xi)} &= A^{(i-1)}_{(\xi)} \cdot \left[\Delta^{(i)}_{(\xi)}\right]^T\\
dB^{(i)}_{(\xi)} &= \Delta^{(i)}_{(\xi)}
\end{align}
\item \textit{update the parameters.}

At this point we introduce the first parameter of the learning process called \texttt{batch\_size}. It states how many votes are processed together to make one update of the parameters. If \texttt{batch\_size = 1}, we are talking about a \textit{sequential learning}. In this case, each vote is applied to update the parameters before any other votes are computed (\cref{eq:sequential_update}; $ t $ refers to a moment in time). 

\begin{align} \label{eq:sequential_update}
dW^{(i)}(t) = dW^{(i)}_{(\xi)}
\end{align}

The other approach is \textit{batch learning}, defined by \texttt{batch\_size > 1}

\begin{align} \label{eq:batch_update}
dW^{(i)}(t) = \displaystyle{\sum_{\xi}^{batch\_size} dW^{(i)}_{(\xi)}}
\end{align}

Equations \ref{eq:sequential_update} and \ref{eq:batch_update} works analogously for the biases.

The second parameter of the learning procedure is called \texttt{learning\_rate} ($ \mu $) and is usually set $ 0 < \mu << 1 $ in order to deal with GDA problems (see \citep{online:nnanddl}). The update of the parameters is then done as follows:
\begin{align} \label{eq:params_update}
W^{(i)} (t+1) &= W^{(i)} (t) + \mu \cdot dW^{(i)} (t) \\ 
B^{(i)} (t+1) &= B^{(i)} (t) + \mu \cdot dB^{(i)} (t)
\end{align}
\end{enumerate}

When the votes of all samples are applied, the learning epoch ends (see \cref{fig:methods:backpropagation}). The maximal number of epochs is the last parameter of the learning procedure. To list them all:

\begin{itemize}
\item \texttt{batch\_size}
\item \texttt{learning\_rate} ($ \mu $)
\item \texttt{n\_epoch}
\end{itemize}

\subsection*{Network Evaluation}
We use two measures to evaluate the network training: error and accuracy.
The error measure is based on the standard \textit{Mean Squared Error (MSE)} given by \cref{eq:mse}.

\begin{equation} \label{eq:mse}
MSE = \frac{1}{2 p} \displaystyle{\sum^{p}_{\xi=1} ||y_{\xi} - u_{\xi}||^2}
\end{equation}

where $ y_{\xi} $ is the prediction for sample $ \xi $ and $ u_{\xi} $ its corresponding (known) target. Both are vectors of length $ m $ (number of classes).

In this study we rather use the measure given by \cref{eq:mse_}, because it makes a fair comparison of problems with different number of classes.

\begin{equation} \label{eq:mse_}
MSE' = \frac{1}{2 p m} \displaystyle \sum^{p}_{\xi=1} \displaystyle \sum^{m}_{\theta=1} (y_{\xi,\theta} - u_{\xi,\theta})^2
\end{equation}

The classification accuracy is computed with \cref{eq:acc}.

\begin{equation} \label{eq:acc}
acc = \frac{1}{p} \displaystyle \sum^{p}_{\xi=1} \psi, \qquad \psi = \begin{cases}
    1, & argmax(y_{\xi}) = argmax(u_{\xi}) \\
    0, & \text{otherwise}
\end{cases} 
\end{equation}

The classification result is often shown by a confusion matrix (well explained in \citep{sklearn}. The testing is usually done on different data samples than used for training - see \cref{fig:methods:three_sets}.

\section{Network Pruning} \label{sec:network_pruning}
The rule of thumb in classification with feedforward neural networks is using a fully connected structure - every unit is connected to all units in the next layer. 

Pruning methods work with the hypothesis that some of the synapses in fully connected networks do not contribute to the classification and so their removal would not cause an accuracy drop. The problem (graphically illustrated in \cref{fig:methods:pruning_problem_formulation}) is to distinguish those redundant synapses from the important ones.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{pruning_problem_formulation}
  \caption{Pruning Algorithm: problem formulation.}
  \label{fig:methods:pruning_problem_formulation}
\end{figure}

There are several ways of how to estimate the importance of synapses (see \cref{sec:state_of_the_art}). In this study we introduce a measure called "\textit{weight significance factor (WSF)} given by \cref{eq:weight_significance_factor}.

\begin{equation} \label{eq:weight_significance_factor}
WSF(w_{k,l}^{(i)}) = | w_{k,l}^{(i)} (t_f) - w_{k,l}^{(i)} (0) |
\end{equation}

where $ w_{k,l}^{(i)} (t_f) $ is the weight of synapse coming to $ k^{th} $ neuron in the $ i^{th} $ layer from $ l^{th} $ neuron in $ (i-1)^{th} $ layer after training ($ t_f $: time final). Analogically, $ w_{k,l}^{(i)} (0) $ is the initial weight of the same synapse before training. We compare these two values and get the change in weight over the training process. The key idea is that the redundant synapses do not change their weights over the training. Therefore, those synapses with low \texttt{WSF} are considered less important than those with high \texttt{WSF}.

\subsection*{Realization of the Pruning Proceeder}


The general workflow consists of the following steps:
\begin{enumerate}
\item a fully connected network is initialized and trained to as high accuracy as possible;
\item some of the synapses are removed and a classification ability of the pruned net is tested;
\item if it has not dropped significantly, the removed synapses were unimportant;
\end{enumerate}

The task is to identify the unimportant synapses. The way of recognizing the importance of synapses is called \textit{pruning algorithm (PA)}.

The idea behind the PA implemented in this study is based on weight changes during the network training. It is presumed that a synapse, whose weight does not evolve while the network is trained, does not contribute to classification much.

\subsection*{Cutting Synapses}
The pruning algorithm itself is an iterative process, however, as shown in \cref{img:pa_flowchart}, two important steps are done in advance.

First of all, the following variables are initialized:

\begin{description}
\item[net] : a fully-connected \textit{KITTNN} \textit{Network()}; (see \cref{app:sec:api_kitt_nn}) with an oversized structure (many hidden neurons) and randomly set weights and biases
\item[percentile] : algorithm variable, set to $ 75 $ by default
\item[percentile levels] : array of final variables, set to $ [75, 50, 20, 5, 1] $ by default
\item[required accuracy] : required classification accuracy for a chosen problem (e.g. 0.95)
\item[stability threshold] : if the classification does not improve over several learning iterations, this is the number of stable iterations to terminate the training after
\end{description}

Additionally, some standard learning parameters like the \textit{learning rate}, \textit{number of epochs} and \textit{mini-batch size} can be set and, of course, a dataset is chosen.

Once the initialization is done, the network is trained with some optimal learning parameters until it reaches the required classification success rate. As mentioned above, the pruning algorithm is based on weight changes. Therefore, the initial weights for synapses are kept. Then, the trained network is passed to the pruning phase, which is shown in \cref{img:pruning_algorithm}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{pruning_algorithm}
  \caption{The Pruning Algorithm: initialized variables are in bold, red marked functions refer to \ref{img:cut_synapses} and \ref{img:accuracy_kept} respectively}
  \label{img:pruning_algorithm}
\end{figure}

Initially, a backup of the current network structure is made by creating a network copy. The pruning is then performed using this copy. The pruning method is shown in \cref{img:cut_synapses}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{cut_synapses}
  \caption{Synaptic pruning based on weight changes and current percentile value}
  \label{img:cut_synapses}
\end{figure}

As discussed above, an initial weight value was saved for each synapse before the training. Hence, for $ n $ being the total number of synapses, weight changes $ \Delta w_i $ are known (\ref{eq:weight_change}).

\begin{align} \label{eq:weight_change}
\Delta w_i = |w\_init_i - w\_current_i|, \quad i = 1, ..., n
\end{align}

Based on these changes and on the current $ percentile $ value, a threshold ($ th $ in \cref{img:cut_synapses}) is determined. Then, all synapses with a lower change in weight than this threshold are removed, and, if there is a neuron with no connections left, it is removed as well.

If the $ percentile $ variable has been decreased so that it equals zero, the threshold is set to the minimum of all weight changes. In this case, only one synapse is then removed at a time.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{accuracy_kept}
  \caption{Evaluation of the classification accuracy after pruning}
  \label{img:accuracy_kept}
\end{figure}

After cutting some synapses, the network is checked for keeping the required classification accuracy as shown in \cref{img:accuracy_kept}.

The evaluation is performed by testing the network on validation data. If the classification accuracy has dropped, the network is retrained using training data of the dataset.

If the classification accuracy has been kept after the pruning, the current net structure is saved and considered as a reference for the next pruning loop.

If it is not possible to retrain the pruned net and to reach the required accuracy, two possibilities arise:

\begin{enumerate}
\item Only one synapse has been removed during the last pruning step and the accuracy has been broken. This means that even this single synapse with the least change in weight is important for classification. Therefore, the pruning is stopped and the current net structure (including this last synapse) is saved as the minimal structure.
\item More synapses have been removed during the last pruning step, and this broke the accuracy. In this case, the percentile level is decreased (based on the initialized array of percentile levels) and so less synapses are removed during the next pruning iteration.

In this manner, at some point of the pruning process, the algorithm will come to removing only one synapse at once and then, finally, only case 1 will remain.
\end{enumerate}

Therefore, the algorithm is finite. Moreover, it guarantees that the classification success rate does not drop. The method is evaluated in detail in \cref{sec:pruning_algorithm_results} and compared to two different approaches from \citep{article:10:pa} in \cref{ssec:comparison_to_other_pa}.

\subsection*{Dimensionality Reduction}

\section{Feature Selection} \label{sec:feature_selection}
Minimal network structure text...

Pathing in Net (Feature Selection)

Feature Energy computation

\newpage
\section{Speech Data Gathering} \label{sec:speech_data_gathering}
The presented methods are tested on several examples (\cref{chap:examples}) and one of them rests in classification of phonemes. By definition a phoneme is one of the units of sound that distinguish one word from another in a particular language \citep{wiki:mnist}. We focus on Czech language and consider $ 40 $ phonemes listed in \cref{tab:methods:phonetic_alphabet}. This section describes the way of gathering such phonemes and building a dataset for classification.

\begin{table}[H]
\centering
\scalebox{0.8}{
\begin{tabular}{|c|c|c||c|c|c||c|c|c|}
\hline
\textit{sound} & \textit{phoneme} & \textit{example} & \textit{sound} & \textit{phoneme} & \textit{example} & \textit{sound} & \textit{phoneme} & \textit{example} \\ \hline \hline
a  & a                & mám\textbf{a}             & ch & x                & \textbf{ch}yba            & ř     & R                & mo\textbf{ř}e             \\ \hline
á  & A                & t\textbf{á}ta             & i  & i                & p\textbf{i}vo             & ř     & Q                & t\textbf{ř}i              \\ \hline
au & Y                & \textbf{au}to             & í  & I                & v\textbf{í}no             & s     & s                & o\textbf{s}el             \\ \hline
b  & b                & \textbf{b}od              & j  & j                & vo\textbf{j}ák            & š     & S                & po\textbf{š}ta            \\ \hline
c  & c                & o\textbf{c}el             & k  & k                & o\textbf{k}o              & t     & t                & o\textbf{t}ec             \\ \hline
č  & C                & o\textbf{č}i              & l  & l                & \textbf{l}oď              & ť     & T                & ku\textbf{t}il            \\ \hline
d  & d                & \textbf{d}ům              & m  & m                & \textbf{m}ír              & u     & u                & r\textbf{u}m              \\ \hline
ď  & D                & \textbf{d}ěti             & n  & n                & \textbf{n}os              & ú (ů) & U                & r\textbf{ů}že             \\ \hline
e  & e                & p\textbf{e}s              & n  & N                & ba\textbf{n}ka            & v     & v                & \textbf{v}lak             \\ \hline
é  & E                & l\textbf{é}pe             & ň  & J                & la\textbf{ň}              & z     & z                & ko\textbf{z}a             \\ \hline
eu & F                & \textbf{eu}nuch           & o  & o                & b\textbf{o}k              & ž     & Z                & \textbf{ž}ena             \\ \hline
f  & f                & \textbf{f}acka            & ou & y                & p\textbf{ou}to            &       & \_sil\_          & (silence)                 \\ \hline
g  & g                & \textbf{g}uma             & p  & p                & \textbf{p}rak             &       &                  &                           \\ \hline
h  & h                & \textbf{h}ad              & r  & r                & \textbf{r}ak              &       &                  &                           \\ \hline
\end{tabular}}
\caption{Czech phonetic alphabet.}
\label{tab:methods:phonetic_alphabet}
\end{table}

The generation of a speech dataset consists of the following steps, where the work done in steps $ 1-3 $ is taken over from \citep{smidl_pc}.

\begin{enumerate}
\item acquisition of real voice recordings;
\item feature extraction from the sound signals (parameterization);
\item labeling the data;
\item definition of one sample;
\item splitting samples into training/development/testing sets.
\end{enumerate}

\subsection*{Acquisition of Voice Recordings}
The phoneme dataset is made of real speech recordings from a car interior environment, provided by (Škoda, ?? ref). We are talking about simple voice instructions for a mobile phone or a navigation system, many of them are names of people, streets or towns only. In total $ 14523 $ recordings (\texttt{.wav} files) of various length (and so number of phonemes) were obtained.

\subsection*{Parameterization}
The goal of parameterization is to represent each recording by a vector of features. A commonly known procedure based on MFCCs is used. A nice detailed explanation of this method can be found e.g. in \citep{online:mfcc}. 

The idea behind MFCCs originates in the fact that a shape of human vocal tract (including tongue, teeth etc.) determines what sound comes out. The shape of the vocal tract manifests itself in the envelope of the short time power spectrum, and the job of MFCCs is to accurately represent this envelope.

The parameterization workflow is summarized by these steps:

\begin{enumerate}
\item \textit{splitting the signal into short frames};

\cref{fig:methods:mfcc_framing} illustrates how a sound signal is splitted into short frames. The parameters are
\begin{align*}
frame\_size &= 0.025 \,s \,= 25 \,ms \\
frame\_shift &= 0.01 \,s \,= 10 \,ms
\end{align*}
Using the sampling frequency $ f_s = 8 kHz $, we get frames of length $ 200 $.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{mfcc_framing.png}
\caption{Framing a sound signal.}
\label{fig:methods:mfcc_framing}
\end{figure}

We assume each frame captures one possible shape of the human vocal tract and therefore is capable of carrying one phoneme only. The next steps are applied for every single frame.

\item \textit{calculation of the periodogram estimate of the power spectrum}; 

The aim is to identify which frequencies are present. In order to do so, we apply the Hamming window and perform the discrete Fourier Transform (DFT; \cref{eq:dft}).

\begin{equation} \label{eq:dft}
S(k) = \displaystyle\sum_{n=0}^{N-1} s_n \cdot e^{-2 \pi i \frac{kn}{N}}, \qquad k = 0, ..., N-1
\end{equation}

, where $ N $ (in this case $ N = 200 $) is the signal length. Then we take the absolute value $ |S(k)| $.

\item \textit{application of the mel filterbank to the power spectra, summation of the energy in each filter, taking a logarithm of the result};

Next, we use a filterbank of triangle filters (illustrated in \cref{fig:methods:mfcc_filterbank}) predefined on the transmitted band ($ bw = \frac{f_s}{2} = 4kHz $) to get a single value per filter. We use $ 40 $ filters, therefore, each frame is now described by a vector of $ 40 $ numbers.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{mfcc_filterbank}
\caption{Mel Filterbank of 40 filters in Hertz-axis.}
\label{fig:methods:mfcc_filterbank}
\end{figure}

Finally, a logarithm of the result is taken and considered as a description of the frame (phoneme). Usually, a discrete Cosine Transform (DCT) is applied at the end, however, it is not done in this work. The result of a signal parameterization is a matrix shown in \cref{eq:parameterization_result}.

\begin{equation} \label{eq:parameterization_result}
recording\_params = 
\begin{bmatrix}
    f_{11} & f_{12} & f_{13} & \dots  & f_{1F} \\
    f_{21} & f_{22} & f_{23} & \dots  & f_{2F} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    f_{W1} & f_{W2} & f_{W3} & \dots  & f_{WF}
\end{bmatrix}
\end{equation}

, where $ F = 40 $ is the number of filters and $ W $ is the number of frames (windows) depending on the duration of a recording. Value $ f_{12} $ then belongs to the feature computed with the second filter in the first frame.
\end{enumerate}

\subsection*{Data Labeling}
We perform a supervised learning method, hence the data must be labeled. To the so a speech recognition method based on Hidden Markov Models (HMMs) from \citep{smidl_pc} is used. It labels the frames of each recording as shown on an example in \cref{tab:methods:labeling_example}.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\multicolumn{3}{|c|}{recording\_name}                                                                                     \\ \hline
\multicolumn{1}{|l|}{\textit{frame\_in}} & \multicolumn{1}{l|}{\textit{frame\_out}} & \multicolumn{1}{l|}{\textit{phoneme}} \\ \hline
0                                       & 16                                      & \_sil\_                               \\ \hline
16                                      & 25                                      & a                                     \\ \hline
25                                      & 32                                      & n                                     \\ \hline
32                                      & 44                                      & o                                     \\ \hline
44                                      & 65                                      & \_sil\_                               \\ \hline
\end{tabular}
\caption{Example of labeled recording.}
\label{tab:methods:labeling_example}
\end{table}

It says that features extracted from this recording consist of $ 9 $ fourty-dimensional vectors representing phoneme \texttt{"a"}, $ 7 $ vectors of phoneme \texttt{"n"}, etc.

\subsection*{Forming a Sample}
The $ 40 $ phonemes listed in \cref{tab:methods:phonetic_alphabet} are naturally labels of classes, so we have a fourty-class classification problem. Having the information from previous section, one can match the extracted features with corresponding phonemes (classes). Now the task is to define the form of one sample.

\cref{fig:methods:sample_forming_bs} goes with the example in \cref{tab:methods:labeling_example}. The numbers in the first line are frame indices. The second line contains the known frame labels, where each frame is described by a vector of $ 40 $ features.

There is a possibility to take all frames labeled as \texttt{"a"} and consider the corresponding vectors directly as samples. However, as the labeling was not done manually and therefore cannot be considered as $ 100\% $ correct, we introduce a parameter called \texttt{border\textunderscore size}. \cref{fig:methods:sample_forming_bs} shows that we omit the frames on borders with another phoneme label and take only those in the middle.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{sample_forming_bs}
\caption{Forming a sample, illustration of parameter \texttt{border\textunderscore size (bs)}.}
\label{fig:methods:sample_forming_bs}
\end{figure}

Moreover, in \cref{fig:methods:sample_forming_cs} parameter \texttt{context\textunderscore size} is introduced. The idea is to consider not only the information of one frame, but also of its context, into one sample.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{sample_forming_cs}
\caption{Forming a sample, illustration of parameter \texttt{context\textunderscore size (cs)}.}
\label{fig:methods:sample_forming_cs}
\end{figure}

Based on the chosen context size $ cs $ the previous and subsequent vectors are added one by one and forms one feature vector of length $ 40 \cdot (2cs+1) $. An example for $ cs = 2 $ is illustrated in \cref{fig:methods:feature_vector}.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{feature_vector}
\caption{Example of building a feature vector with \texttt{context\textunderscore size} $ cs = 2 $.}
\label{fig:methods:feature_vector}
\end{figure}

Talking about \cref{fig:methods:feature_vector}, features $ g_1, g_2, ..., g_{200} $ gives the final feature vector of one sample, which takes the label of the base frame.

The last parameter of the speech dataset generation is the number of samples per class (\texttt{n\textunderscore samples}). The rule of thumb is the more samples the better training results, however, getting best possible training results is not the objective of this work. Therefore we often use less samples to speed up the training process.

To summarize this section, we end up with three parameters of the speech dataset generation process:

\begin{itemize}
\item \texttt{border\textunderscore size} (see \cref{fig:methods:sample_forming_bs})
\item \texttt{context\textunderscore size} (see \cref{fig:methods:sample_forming_cs})
\item \texttt{n\textunderscore samples} per class
\end{itemize}

\subsection*{Splitting data into three disjunctive sets}
\cref{fig:methods:three_sets} shows a general approach of data splitting in machine learning. It is used for all classification problems in this work. The training data is used to set up model parameters. Development data is then used for testing during the training process, in order to adjust some learning parameters based on the test results. Finally, a trained model is tested on never-seen testing data. We use splitting: $ 80\% $ training set; $ 10\% $ development set; $ 10\% $ testing set.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{three_sets}
\caption{Using three disjunctive sets of data for a general machine learning process.}
\label{fig:methods:three_sets}
\end{figure}