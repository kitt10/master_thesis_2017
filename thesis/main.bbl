% $ biblatex auxiliary file $
% $ biblatex version 2.6 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\entry{article:perceptron}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Rosenblatt}{R.}%
     {Frank}{F.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{RF1}
  \strng{fullhash}{RF1}
  \field{labelyear}{1958}
  \field{sortinit}{1}
  \field{pages}{386\bibrangedash 408}
  \field{title}{The perceptron: A probabilistic model for information storage
  and organization in the brain}
  \field{volume}{65}
  \field{journaltitle}{Psychological Review}
  \field{year}{1958}
\endentry

\entry{michalski}{inproceedings}{}
  \name{author}{2}{}{%
    {{}%
     {Larson}{L.}%
     {J.}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Michalski}{M.}%
     {R.~S.}{R.~S.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {ACM SIGAI}%
  }
  \strng{namehash}{LJMRS1}
  \strng{fullhash}{LJMRS1}
  \field{labelyear}{1977}
  \field{sortinit}{1}
  \field{booktitle}{ACM SIGART Bulletin}
  \field{pages}{38\bibrangedash 44}
  \field{title}{Inductive Inference of VL Decision Rules}
  \list{location}{1}{%
    {New York, USA}%
  }
  \field{year}{1977}
\endentry

\entry{mozer_smolensky}{inproceedings}{}
  \name{author}{2}{}{%
    {{}%
     {Mozer}{M.}%
     {Michael~C.}{M.~C.}%
     {}{}%
     {}{}}%
    {{}%
     {Smolensky}{S.}%
     {Paul}{P.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {CO 80309-0430}%
  }
  \strng{namehash}{MMCSP1}
  \strng{fullhash}{MMCSP1}
  \field{labelyear}{1989}
  \field{sortinit}{1}
  \field{booktitle}{Boulder}
  \field{pages}{107\bibrangedash 115}
  \field{title}{Skeletonization: A Technique for Trimming the Fat from a
  Network via Relevance Assessment}
  \list{location}{1}{%
    {Institute of Cognitive Science, University of Colorado}%
  }
  \field{year}{1989}
\endentry

\entry{karnin}{inproceedings}{}
  \name{author}{1}{}{%
    {{}%
     {Karnin}{K.}%
     {Ehud~D.}{E.~D.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {IEEE Transaction on Neural Networks}%
  }
  \strng{namehash}{KED1}
  \strng{fullhash}{KED1}
  \field{labelyear}{1990}
  \field{sortinit}{1}
  \field{booktitle}{Letters}
  \field{pages}{239\bibrangedash 242}
  \field{title}{A Simple Procedure for Pruning Back-Propagation Trained Neural
  Networks}
  \field{year}{1990}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{article:pruning_algorithms}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Reed}{R.}%
     {R.}{R.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{RR1}
  \strng{fullhash}{RR1}
  \field{labelyear}{1993}
  \field{sortinit}{1}
  \field{abstract}{%
  A rule of thumb for obtaining good generalization in systems trained by
  examples is that one should use the smallest system that will fit the data.
  Unfortunately, it usually is not obvious what size is best; a system that is
  too small will not be able to learn the data while one that is just big
  enough may learn very slowly and be very sensitive to initial conditions and
  learning parameters. This paper is a survey of neural network pruning
  algorithms. The approach taken by the methods described here is to train a
  network that is larger than necessary and then remove the parts that are not
  needed.%
  }
  \field{pages}{740\bibrangedash 747}
  \field{title}{Pruning Algorithms - A Survey}
  \verb{url}
  \verb http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=248452
  \endverb
  \field{journaltitle}{IEEE Transactions on Neural Networks (Volume:4 , Issue:
  5)}
  \field{month}{09}
  \field{year}{1993}
\endentry

\entry{online:mnist}{online}{}
  \name{author}{2}{}{%
    {{}%
     {LeCun}{L.}%
     {Yann}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Cortes}{C.}%
     {Corinna}{C.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{LYCC1}
  \strng{fullhash}{LYCC1}
  \field{labelyear}{1998}
  \field{sortinit}{1}
  \field{title}{The MNIST database of handwritten digits}
  \verb{url}
  \verb http://yann.lecun.com/exdb/mnist/
  \endverb
  \field{year}{1998}
\endentry

\entry{wiki:mnist}{misc}{}
  \name{author}{1}{}{%
    {{}%
     {Wikipedia}{W.}%
     {}{}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{W1}
  \strng{fullhash}{W1}
  \field{labelyear}{2004}
  \field{sortinit}{2}
  \field{note}{[Online; accessed 15-April-2017]}
  \field{title}{Plagiarism --- {W}ikipedia{,} The Free Encyclopedia}
  \verb{url}
  \verb https://en.wikipedia.org/wiki/MNIST_database
  \endverb
  \field{year}{2004}
\endentry

\entry{online:xor_solution}{online}{}
  \name{author}{1}{}{%
    {{}%
     {Bradley}{B.}%
     {Peter}{P.}%
     {}{}%
     {}{}}%
  }
  \strng{namehash}{BP1}
  \strng{fullhash}{BP1}
  \field{labelyear}{2006}
  \field{sortinit}{2}
  \field{title}{The XOR Problem and Solution}
  \verb{url}
  \verb http://www.mind.ilstu.edu/curriculum/artificial_neural_net/xor_problem_
  \verb and_solution.php
  \endverb
  \field{year}{2006}
\endentry

\entry{online:census}{online}{}
  \field{labelyear}{2017}
  \field{sortinit}{2}
  \field{title}{United States Census Bureau}
  \verb{url}
  \verb https://www.census.gov/
  \endverb
  \field{year}{2017}
\endentry

\lossort
\endlossort

\endinput
