@article{article:pruning_algorithms,
	Abstract = {A rule of thumb for obtaining good generalization in systems trained by examples is that one should use the smallest system that will fit the data. Unfortunately, it usually is not obvious what size is best; a system that is too small will not be able to learn the data while one that is just big enough may learn very slowly and be very sensitive to initial conditions and learning parameters. This paper is a survey of neural network pruning algorithms. The approach taken by the methods described here is to train a network that is larger than necessary and then remove the parts that are not needed.},
	Author = {R. Reed},
	Journal = {IEEE Transactions on Neural Networks  (Volume:4 ,  Issue: 5)},
	Month = {9},
	Numpages = {7},
	Pages = {740--747},
	Title = {Pruning Algorithms - A Survey},
	Url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=248452},
	Year = {1993}}
	
@online{online:mnist,
        title = {The MNIST database of handwritten digits},
        date = {1998},
        author = {Yann LeCun and Corinna Cortes},
        url = {http://yann.lecun.com/exdb/mnist/}
}

@article{article:perceptron,
 title={The perceptron: A probabilistic model for information storage and organization in the brain},
 author={Frank Rosenblatt},
 journal={Psychological Review},
 volume={65},
 pages={386--408},
 year={1958}
}